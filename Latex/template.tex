\documentclass[12pt]{article}
\usepackage[UTF8]{ctex}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{times}
\usepackage{setspace}
\usepackage{abstract}
\usepackage{titlesec}
\usepackage{appendix}

% 页面设置
\geometry{a4paper, left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}
\setstretch{1.5}

% 修改章节格式设置 - 调整大小
\titleformat{\section}
{\LARGE\bfseries\centering}{\thesection}{1em}{}
\titleformat{\subsection}
{\Large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}
{\large\bfseries}{\thesubsubsection}{1em}{}

% 代码样式设置
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single,
    rulecolor=\color{black}
}

\begin{document}

% 自定义标题页
\begin{titlepage}
    \centering
    
    % 第一行：插入西安交通大学Logo
    \includegraphics[width=0.6\textwidth]{XJTU.png}
    \vspace{1cm}
    
    % 第二行：管理学院和课程信息
    {\LARGE\bfseries 管理学院\par}
    \vspace{0.3cm}
    {\LARGE\bfseries 《最优化理论与算法ll》课程小组作业—1\par}
    \vspace{1.2cm}
    
    % 第三行：标题
    {\Large\bfseries LASSO 回归算法实现与比较分析\par}
    \vspace{1.5cm}
    
    % 第四部分：小组成员信息
    {\large
    \begin{tabular}{l l}
        \textbf{陈吉龙} & \textbf{2236115452} \\
        \textbf{PUA WENG YANG} & \textbf{2229990011} \\
        \textbf{ROMANOV ARTEM} & \textbf{2239990071} \\
    \end{tabular}
    }
    \vfill
    
    {\large \today\par}
\end{titlepage}

% 摘要
\begin{abstract}
本实验实现了求解LASSO回归问题的三种优化算法：次梯度法（Subgradient Method）、临近点梯度法（Proximal Gradient Method）和交替方向乘子法（ADMM）。针对不同的样本维度组合$(n,p)$，我们系统比较了各算法的收敛性能、计算效率和实现特点。实验结果表明，临近点梯度法在大多数情况下具有最佳的收敛速度，ADMM算法在特定问题配置下表现优越，而次梯度法作为基础方法虽然收敛较慢但实现简单。通过详细分析算法实现细节与原算法的差异，我们进一步探讨了优化策略的有效性。
\end{abstract}

\section{引言}

\subsection{LASSO问题背景}
LASSO（Least Absolute Shrinkage and Selection Operator）回归由Tibshirani于1996年提出，是一种在回归分析中同时进行特征选择和正则化的统计方法。LASSO问题可表述为：

\begin{equation}
\min_{x} \frac{1}{2} \|Ax - b\|_2^2 + \lambda \|x\|_1
\end{equation}

其中$A \in \mathbb{R}^{n \times p}$为设计矩阵，$b \in \mathbb{R}^n$为响应向量，$\lambda > 0$为正则化参数。L1正则项$\|x\|_1$的引入使得解具有稀疏性，这是LASSO方法的核心特征。

\subsection{算法研究意义}
由于目标函数中包含不可微的L1范数项，LASSO问题的求解需要专门的优化算法。本实验通过实现三种代表性算法，深入探讨：
\begin{enumerate}
    \item 不同算法在LASSO问题上的收敛特性
    \item 算法实现细节对性能的影响
    \item 问题规模$(n,p)$对算法表现的影响
\end{enumerate}

\section{算法原理与实现}

\subsection{次梯度法（Subgradient Method）}

\subsubsection{算法原理}
次梯度法是处理非光滑优化问题的基本方法。对于LASSO问题，目标函数的次梯度为：

\begin{equation}
\partial f(x) = A^T(Ax - b) + \lambda \partial \|x\|_1
\end{equation}

其中$\partial \|x\|_1$是L1范数的次微分：
\begin{equation}
\partial |x_i| = 
\begin{cases}
\{1\} & x_i > 0 \\
\{-1\} & x_i < 0 \\
[-1, 1] & x_i = 0
\end{cases}
\end{equation}

迭代更新公式为：
\begin{equation}
x^{k+1} = x^k - \alpha_k g^k, \quad g^k \in \partial f(x^k)
\end{equation}

\subsubsection{实现特点与原算法差异}
\begin{itemize}
    \item \textbf{步长策略}：代码中使用$\alpha_k = 1/(k + \mu)$，其中$\mu = \lambda_{\max}(A^T A)$，这是对经典步长$1/\sqrt{k}$的改进
    \item \textbf{两阶段执行}：第一阶段寻找近似最优值$f^*$，第二阶段记录收敛过程
    \item \textbf{次梯度计算优化}：通过阈值判断处理L1范数的次微分，避免了复杂的集合运算
\end{itemize}

\subsubsection{核心代码实现}
\begin{lstlisting}[language=Python, caption=次梯度法核心代码]
def SubGradMethod(A, b, Lambda):
    """次梯度法求解LASSO问题"""
    Accuracy, Time, F = [], [], []
    for i in range(10):
        p = len(A[0][0])
        # 第一阶段：寻找最优值f_star
        time, x1, f_star = 0, np.full((p, 1), 0), 10000000
        Miu = max(np.linalg.eig(np.dot(A[i], A[i].T))[0])
        A1, b1 = A[i], b[i]
        
        while time < 40000:
            time += 1
            # 计算次梯度
            SubGrad1 = np.full((p, 1), 0)
            for j in range(p):
                if x1[j][0] > 0:
                    SubGrad1[j][0] = 1
                elif x1[j][0] < -0:
                    SubGrad1[j][0] = -1
                else:
                    SubGrad1[j][0] = 0
            
            SubGrad2 = np.dot(A1.T, np.dot(A1, x1) - b1) + Lambda * SubGrad1
            # 更新x
            x1 = x1 - (1 / (time + Miu)) * SubGrad2
            # 计算目标函数值
            f = 0.5 * np.linalg.norm(np.dot(A1, x1) - b1, ord=2) + Lambda * np.linalg.norm(x1, ord=1)
            if f < f_star:
                f_star = f
\end{lstlisting}

\subsection{临近点梯度法（Proximal Gradient Method）}

\subsubsection{算法原理}
临近点梯度法将LASSO问题分解为光滑部分和非光滑部分：
\begin{equation}
f(x) = g(x) + h(x)
\end{equation}
其中$g(x) = \frac{1}{2}\|Ax - b\|_2^2$为光滑部分，$h(x) = \lambda\|x\|_1$为非光滑部分。

迭代更新分为两步：
\begin{align}
y^{k+1} &= x^k - \alpha_k \nabla g(x^k) \\
x^{k+1} &= \text{prox}_{\alpha_k h}(y^{k+1}) = \text{sign}(y^{k+1}) \odot \max(|y^{k+1}| - \lambda\alpha_k, 0)
\end{align}

其中软阈值算子$\text{prox}_{\alpha h}(y)$解析形式为：
\begin{equation}
[\text{prox}_{\alpha h}(y)]_i = \text{sign}(y_i) \cdot \max(|y_i| - \lambda\alpha, 0)
\end{equation}

\subsubsection{实现特点与原算法差异}
\begin{itemize}
    \item \textbf{步长选择}：采用$\alpha = 1/L$，其中$L = \lambda_{\max}(A^T A)$是$\nabla g(x)$的Lipschitz常数
    \item \textbf{向量化实现}：使用NumPy的向量化操作实现软阈值算子，提高计算效率
    \item \textbf{收敛保证}：步长选择确保了算法的收敛性，收敛速度为$O(1/k)$
\end{itemize}

\subsubsection{核心代码实现}
\begin{lstlisting}[language=Python, caption=临近点梯度法核心代码]
def ProximalGradMethod(A, b, Lambda):
    """临近点梯度法求解LASSO问题"""
    p = len(A[0][0])
    Miu = 1 / max(np.linalg.eig(np.dot(A[i], A[i].T))[0])
    
    # 梯度下降步
    Grad = np.dot(A1.T, np.dot(A1, x1) - b1)
    y1 = x1 - Miu * Grad
    
    # 软阈值操作（临近算子）
    x1_star = np.sign(y1) * np.maximum(np.abs(y1) - Lambda * Miu, 0)
    x1 = x1_star
    
    # 计算目标函数值
    f = np.linalg.norm(0.5 * (np.dot(A1, x1) - b1), ord=2) + Lambda * np.linalg.norm(x1, ord=1)
\end{lstlisting}

\subsection{交替方向乘子法（ADMM）}

\subsubsection{算法原理}
ADMM通过引入辅助变量$z$将问题重构为：
\begin{align}
\min_{x,z} & \quad \frac{1}{2}\|Ax - b\|_2^2 + \lambda\|z\|_1 \\
\text{s.t.} & \quad x - z = 0
\end{align}

增广拉格朗日函数为：
\begin{equation}
L_\rho(x,z,y) = \frac{1}{2}\|Ax - b\|_2^2 + \lambda\|z\|_1 + y^T(x-z) + \frac{\rho}{2}\|x-z\|_2^2
\end{equation}

迭代更新公式为：
\begin{align}
x^{k+1} &= (A^T A + \rho I)^{-1}(A^T b + \rho(z^k - y^k)) \\
z^{k+1} &= S_{\lambda/\rho}(x^{k+1} + y^k) \\
y^{k+1} &= y^k + x^{k+1} - z^{k+1}
\end{align}
其中$S_\kappa(a) = \text{sign}(a) \odot \max(|a| - \kappa, 0)$为软阈值算子。

\subsubsection{实现特点与原算法差异}
\begin{itemize}
    \item \textbf{矩阵逆缓存}：预先计算$(A^T A + \rho I)^{-1}$避免重复求逆
    \item \textbf{参数选择}：惩罚参数$\rho = 0.1$，需根据问题特性调整
    \item \textbf{实现注意}：代码中软阈值参数为$\mu/\rho$而非$\lambda/\rho$，可能存在计算偏差
\end{itemize}

\subsubsection{核心代码实现}
\begin{lstlisting}[language=Python, caption=ADMM算法核心代码]
def ADMM(A, b, Lambda):
    """ADMM算法求解LASSO问题"""
    p = len(A[0][0])
    Rou = 0.1  # 惩罚参数
    Rev = np.linalg.inv(np.dot(A1.T, A1) + Rou * np.eye(p))
    
    # ADMM更新步骤
    x1 = np.dot(Rev, np.dot(A1.T, b1) + Rou * (z - y))
    z = np.sign(x1 + Rou * y) * np.maximum(np.abs(x1 + Rou * y) - Miu / Rou, 0)
    y = y + x1 - z
\end{lstlisting}

\section{实验设计与实现}

\subsection{数据生成}

\subsubsection{数据生成函数}
代码使用\texttt{QuestionGenerate}函数生成测试数据：
\begin{itemize}
    \item 设计矩阵$A$：元素在$[-5, 5]$均匀分布
    \item 参数向量$x$：元素在$[-10, 10]$整数均匀分布
    \item 响应向量$b$：通过线性模型$b = Ax$生成
\end{itemize}

\subsubsection{实验配置}
测试了四种$(n, p)$组合：
\begin{enumerate}
    \item $(10, 5)$：小样本、低维
    \item $(100, 5)$：大样本、低维
    \item $(10, 50)$：小样本、高维
    \item $(10, 500)$：小样本、超高维
\end{enumerate}

每个配置进行10次独立实验，正则化参数$\lambda = 1$。

\subsection{评估指标}

\subsubsection{收敛精度}
定义相对误差为：
\begin{equation}
\text{Relative Error} = \log_{10}\left(\frac{f^k - f^*}{f^*}\right)
\end{equation}
其中$f^k$为第$k$次迭代的目标函数值，$f^*$为近似最优值。

\subsubsection{迭代过程}
记录每次迭代的：
\begin{itemize}
    \item 迭代次数$k$
    \item 目标函数值$f^k$
    \item 相对误差
\end{itemize}

\section{实验结果与分析}

\subsection{收敛性能比较}

\subsubsection{次梯度法收敛特性}
\begin{itemize}
    \item \textbf{收敛速度}：典型收敛速度为$O(1/\sqrt{k})$，较慢
    \item \textbf{步长影响}：步长$\alpha_k = 1/(k+\mu)$相比经典步长$1/\sqrt{k}$在早期迭代中步长较大，可能加速初期收敛
    \item \textbf{震荡现象}：由于次梯度的不连续性，收敛曲线可能出现震荡
\end{itemize}

\subsubsection{临近点梯度法收敛特性}
\begin{itemize}
    \item \textbf{收敛速度}：理论收敛速度为$O(1/k)$，优于次梯度法
    \item \textbf{平滑收敛}：临近算子的使用使得收敛曲线更加平滑
    \item \textbf{步长优势}：固定步长$1/L$保证了算法的稳定收敛
\end{itemize}

\subsubsection{ADMM收敛特性}
\begin{itemize}
    \item \textbf{收敛速度}：通常为线性收敛，速度介于次梯度法和临近点梯度法之间
    \item \textbf{参数敏感性}：收敛性能对惩罚参数$\rho$敏感
    \item \textbf{内存需求}：需要存储和求逆$p \times p$矩阵，内存需求较高
\end{itemize}

\subsection{不同$(n,p)$组合下的表现}

% 第一部分：n=10, p=5
\subsubsection{$(10, 5)$小样本低维情况}
\begin{itemize}
    \item 所有算法都能快速收敛
    \item 临近点梯度法收敛最快
    \item 次梯度法由于问题简单也表现良好
\end{itemize}

% 插入(10,5)的图片 - 次梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_2.png}
\caption{次梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_1.png}
\caption{次梯度法目标函数值}
\end{subfigure}
\caption{$(10,5)$次梯度法收敛结果}
\end{figure}

% 插入(10,5)的图片 - 临近点梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_4.png}
\caption{临近点梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_3.png}
\caption{临近点梯度法目标函数值}
\end{subfigure}
\caption{$(10,5)$临近点梯度法收敛结果}
\end{figure}

% 插入(10,5)的图片 - ADMM
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_5.png}
\caption{ADMM收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_6.png}
\caption{ADMM目标函数值}
\end{subfigure}
\caption{$(10,5)$ ADMM收敛结果}
\end{figure}

% 第二部分：n=100, p=5
\subsubsection{$(100, 5)$大样本低维情况}
\begin{itemize}
    \item 样本增加但维度不变
    \item 次梯度法受样本量影响较小
    \item ADMM需要求逆的矩阵维度不变，性能稳定
\end{itemize}

% 插入(100,5)的图片 - 次梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_8.png}
\caption{次梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_7.png}
\caption{次梯度法目标函数值}
\end{subfigure}
\caption{$(100,5)$次梯度法收敛结果}
\end{figure}

% 插入(100,5)的图片 - 临近点梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_10.png}
\caption{临近点梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_9.png}
\caption{临近点梯度法目标函数值}
\end{subfigure}
\caption{$(100,5)$临近点梯度法收敛结果}
\end{figure}

% 插入(100,5)的图片 - ADMM
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_11.png}
\caption{ADMM收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_12.png}
\caption{ADMM目标函数值}
\end{subfigure}
\caption{$(100,5)$ ADMM收敛结果}
\end{figure}

% 第三部分：n=10, p=50
\subsubsection{$(10, 50)$小样本高维情况}
\begin{itemize}
    \item 维度增加显著影响算法性能
    \item 次梯度法收敛变慢
    \item ADMM需要求逆50×50矩阵，计算成本增加
\end{itemize}

% 插入(10,50)的图片 - 次梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_13.png}
\caption{次梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_14.png}
\caption{次梯度法目标函数值}
\end{subfigure}
\caption{$(10,50)$次梯度法收敛结果}
\end{figure}

% 插入(10,50)的图片 - 临近点梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_15.png}
\caption{临近点梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_16.png}
\caption{临近点梯度法目标函数值}
\end{subfigure}
\caption{$(10,50)$临近点梯度法收敛结果}
\end{figure}

% 插入(10,50)的图片 - ADMM
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_17.png}
\caption{ADMM收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_18.png}
\caption{ADMM目标函数值}
\end{subfigure}
\caption{$(10,50)$ ADMM收敛结果}
\end{figure}

% 第四部分：n=10, p=500
\subsubsection{$(10, 500)$小样本超高维情况}
\begin{itemize}
    \item 维度远大于样本数
    \item 次梯度法仍可工作但收敛极慢
    \item ADMM需要求逆500×500矩阵，计算成本极高
    \item 临近点梯度法受影响最小，仅需计算梯度
\end{itemize}

% 插入(10,500)的图片 - 次梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_19.png}
\caption{次梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_20.png}
\caption{次梯度法目标函数值}
\end{subfigure}
\caption{$(10,500)$次梯度法收敛结果}
\end{figure}

% 插入(10,500)的图片 - 临近点梯度法
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_21.png}
\caption{临近点梯度法收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_22.png}
\caption{临近点梯度法目标函数值}
\end{subfigure}
\caption{$(10,500)$临近点梯度法收敛结果}
\end{figure}

% 插入(10,500)的图片 - ADMM
\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_23.png}
\caption{ADMM收敛精度}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figure_24.png}
\caption{ADMM目标函数值}
\end{subfigure}
\caption{$(10,500)$ ADMM收敛结果}
\end{figure}

\subsection{算法实现优化分析}

\subsubsection{次梯度法优化点}
\begin{enumerate}
    \item \textbf{步长策略优化}：结合问题特征值信息调整步长
    \item \textbf{两阶段策略}：分离最优值搜索和收敛记录
    \item \textbf{数值稳定性}：设置阈值避免除零错误
\end{enumerate}

\subsubsection{临近点梯度法优化点}
\begin{enumerate}
    \item \textbf{向量化实现}：软阈值算子完全向量化
    \item \textbf{Lipschitz常数计算}：预计算并重用特征值
    \item \textbf{内存效率}：无需存储大型矩阵
\end{enumerate}

\subsubsection{ADMM优化点}
\begin{enumerate}
    \item \textbf{矩阵逆缓存}：避免重复求逆
    \item \textbf{变量更新顺序}：优化更新顺序减少计算
    \item \textbf{并行化潜力}：部分更新可并行进行
\end{enumerate}

\subsection{与经典算法的差异总结}

\begin{table}[H]
\centering
\caption{算法实现与经典版本的差异总结}
\label{tab:algorithm_differences}
\begin{tabular}{p{0.2\textwidth}p{0.35\textwidth}p{0.35\textwidth}}
\toprule
\textbf{算法} & \textbf{经典版本特点} & \textbf{本实现优化点} \\
\midrule
次梯度法 & 步长$\alpha_k = c/\sqrt{k}$或$c/k$ & $\alpha_k = 1/(k+\mu)$，$\mu$为特征值 \\
& 单阶段执行 & 两阶段：先找$f^*$，再记录收敛 \\
& 直接计算次梯度 & 阈值处理次微分，避免集合运算 \\
\hline
临近点梯度法 & 步长需满足$\alpha < 1/L$ & 直接取$\alpha = 1/L$ \\
& 软阈值逐元素计算 & 完全向量化实现 \\
& 可能需线搜索 & 固定步长，无需线搜索 \\
\hline
ADMM & 软阈值参数$\lambda/\rho$ & 代码中使用$\mu/\rho$（可能为笔误） \\
& 需每次迭代求解线性系统 & 预计算矩阵逆并缓存 \\
& $\rho$需调优 & 固定$\rho=0.1$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{实验结论}
基于对三种算法在四种不同$(n,p)$组合下的实验结果分析，我们得出以下主要结论：

\begin{enumerate}
    \item \textbf{算法收敛速度}：临近点梯度法在大多数情况下收敛最快，ADMM在特定配置下表现良好，次梯度法收敛最慢但实现最简单
    
    \item \textbf{问题规模影响}：
    \begin{itemize}
        \item 对于低维问题$(p=5)$，所有算法都能有效工作
        \item 对于高维问题$(p=50,500)$，临近点梯度法优势明显
        \item 当$p$很大时$(p=500)$，ADMM的计算成本显著增加
    \end{itemize}
    
    \item \textbf{内存效率}：临近点梯度法内存需求最低，ADMM需要存储和求逆大规模矩阵
    
    \item \textbf{实现复杂度}：次梯度法实现最简单，ADMM实现最复杂但理论性质最好
    
    \item \textbf{实际应用建议}：
    \begin{itemize}
        \item 对于小规模问题，临近点梯度法是最佳选择
        \item 对于大规模问题，临近点梯度法仍能保持较好性能
        \item 次梯度法适合作为基准方法或教学演示
        \item ADMM适合分布式计算环境或特定结构问题
    \end{itemize}
\end{enumerate}

\appendix
\section{附录：完整代码实现}

\subsection{主程序结构}
\begin{lstlisting}[language=Python, caption=主程序结构]
def main():
    """主函数，运行不同配置的LASSO问题"""
    print("图像会在运行结束后一起出现")   
    # 配置列表：[(n, p), ...]
    configs = [(10, 5), (100, 5), (10, 50), (10, 500)]
    Lambda = 1  # 正则化参数
    
    for n, p in configs:
        print(f"\n对于n = {n}, p = {p}的Lasso问题的次梯度法迭代结果:")
        A, x, b = QuestionGenerate(n, p)
        Accuracy_SG, Time_SG, F_SG = SubGradMethod(A, b, Lambda)
        plot(Accuracy_SG, Time_SG, F_SG, n, p, "次梯度法")
        
        print(f"对于n = {n}, p = {p}的Lasso问题的临近点梯度法迭代结果:")
        Accuracy_PG, Time_PG, F_PG = ProximalGradMethod(A, b, Lambda)
        plot(Accuracy_PG, Time_PG, F_PG, n, p, "临近点梯度法")
        
        print(f"对于n = {n}, p = {p}的Lasso问题的ADMM迭代结果:")
        Accuracy_ADMM, Time_ADMM, F_ADMM = ADMM(A, b, Lambda)
        plotADMM(Accuracy_ADMM, Time_ADMM, F_ADMM, n, p)
        
        print("图像会在运行结束后一起出现")
    
    # 显示所有图像
    plt.show()
\end{lstlisting}

\subsection{数据生成函数}
\begin{lstlisting}[language=Python, caption=数据生成函数]
def QuestionGenerate(n, p):
    """
    问题生成函数
    生成测试数据A, x, b
    """
    A, x, b = [], [], []
    for i in range(testsize):
        np.random.seed(i)
        A.append((np.random.rand(n, p) - 0.5) * 10)
        x.append(np.random.randint(-10, 10, (p, 1)))
        b.append(np.dot(A[i], x[i]))
    return A, x, b
\end{lstlisting}

\subsection{可视化函数}
\begin{lstlisting}[language=Python, caption=通用绘图函数]
def plot(Accuracy, Time, F, n, p, Method):
    """通用绘图函数"""
    # 绘制精度图
    plt.figure(figsize=(48, 20))
    plt.title(f"对于n = {n}, p = {p}的Lasso问题的{Method}迭代结果:")
    plt.ylabel("log10(f^k-f*)/f*")
    plt.xlabel("k")
    
    for i in range(testsize):
        color_val = 25 * i + 25
        hex_str = format(color_val, '02x')
        plt.plot(Time[i], Accuracy[i], f"#00{hex_str}{hex_str}")   
    # 绘制目标函数值图
    plt.figure(figsize=(24, 10))
    plt.title(f"对于n = {n}, p = {p}的Lasso问题的{Method}迭代结果:")
    plt.ylabel("||Ax^k-b||^2+λ|x|")
    plt.xlabel("k")
    
    for i in range(testsize):
        color_val = 25 * i + 25
        hex_str = format(color_val, '02x')
        plt.plot(Time[i], F[i], f"#00{hex_str}{hex_str}")
\end{lstlisting}

\end{document}